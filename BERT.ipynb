{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntZH_CslafnW"
   },
   "outputs": [],
   "source": [
    "def clean_wikitext(line):\n",
    "  line = line.strip()\n",
    "\n",
    "  if line.startswith(\"=\") and line.endswith(\"=\"):\n",
    "    return None\n",
    "\n",
    "  line = line.replace(\"@-@\",\"-\")\n",
    "\n",
    "  line = \" \".join(line.split())\n",
    "\n",
    "  return line if len(line) > 10 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtaS_3hFbGDa"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/BERT/train.txt\") as f:\n",
    "  raw_lines = f.readlines()\n",
    "\n",
    "texts = []\n",
    "for line in raw_lines:\n",
    "  clean = clean_wikitext(line)\n",
    "  if clean:\n",
    "    texts.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqFLAB8xba0w",
    "outputId": "51810c75-879a-44e5-f025-d52389daac40"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  print(texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52ACv7aegp4-"
   },
   "source": [
    "#### Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFWgZKkudfBN"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKEyuxNsffix"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShZ64mKzfoPe"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LR = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vySr50rHgulX"
   },
   "source": [
    "### Simple Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTGdKdV9fpU9"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, texts, vocab_size):\n",
    "    words = set()\n",
    "    for text in texts:\n",
    "      words.update(text.split())\n",
    "\n",
    "    words = list(words)[: vocab_size - 5]\n",
    "\n",
    "    self.word2id = {\n",
    "        \"[PAD]\":0,\n",
    "        \"[CLS]\":1,\n",
    "        \"[SEP]\":2,\n",
    "        \"[MASK]\":3,\n",
    "        \"<unk>\":4\n",
    "    }\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "      self.word2id[w] = i + 4\n",
    "\n",
    "    self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    return [self.word2id.get(w, self.word2id[\"<unk>\"]) for w in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-L0QnL1MIpN"
   },
   "source": [
    "### MLM Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeNzls3ajwBc"
   },
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "  def __init__(self, texts, tokenizer):\n",
    "    self.texts = texts\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def mask_tokens(self, tokens):\n",
    "    labels = [-100]* len(tokens)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "      if random.random()< 0.15:\n",
    "        labels[i] = tokens[i]\n",
    "\n",
    "        prob = random.random()\n",
    "        if prob < 0.8:\n",
    "          tokens[i] = self.tokenizer.word2id[\"[MASK]\"]\n",
    "        elif prob < 0.9:\n",
    "          tokens[i] = random.randint(0, VOCAB_SIZE - 1)\n",
    "    return tokens, labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    tokens = self.tokenizer.encode(self.texts[idx])\n",
    "    tokens = tokens[:SEQ_LEN - 2]\n",
    "\n",
    "    tokens = (\n",
    "        [self.tokenizer.word2id[\"[CLS]\"]]\n",
    "        + tokens\n",
    "        + [self.tokenizer.word2id[\"[SEP]\"]]\n",
    "    )\n",
    "\n",
    "    tokens, labels = self.mask_tokens(tokens)\n",
    "\n",
    "    padding = SEQ_LEN - len(tokens)\n",
    "    tokens += [0] * padding\n",
    "    labels += [-100] * padding # Pad labels with ignore_index\n",
    "\n",
    "    return (\n",
    "        torch.tensor(tokens),\n",
    "        torch.tensor(labels)\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKQsh4VaN21c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfa3612d",
    "outputId": "afc4e00c-5964-49bd-942e-fb807be78321"
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer(texts, VOCAB_SIZE)\n",
    "mlm_dataset = MLMDataset(texts, tokenizer)\n",
    "\n",
    "# Let's take the first item from the dataset as an example\n",
    "index_to_sample = 0\n",
    "sampled_tokens, sampled_labels = mlm_dataset[index_to_sample]\n",
    "\n",
    "# Decode tokens and labels for better understanding\n",
    "original_text = texts[index_to_sample]\n",
    "\n",
    "# To see the original tokens before masking, we'd need to re-encode the text\n",
    "# For this example, let's focus on what the dataset outputs\n",
    "\n",
    "print(f\"Original text: {original_text}\\n\")\n",
    "\n",
    "print(\"Masked tokens (numerical IDs):\\n\", sampled_tokens.tolist())\n",
    "print(\"Labels (numerical IDs, -100 for unmasked):\\n\", sampled_labels.tolist())\n",
    "\n",
    "print(\"\\n--- Decoded Example ---\")\n",
    "decoded_masked_tokens = [tokenizer.id2word.get(token_id.item(), '<unk>') for token_id in sampled_tokens]\n",
    "decoded_labels = []\n",
    "for i, label_id in enumerate(sampled_labels):\n",
    "    if label_id.item() != -100:\n",
    "        decoded_labels.append(f\"({decoded_masked_tokens[i]} -> {tokenizer.id2word.get(label_id.item(), '<unk>')})\")\n",
    "    else:\n",
    "        decoded_labels.append(tokenizer.id2word.get(sampled_tokens[i].item(), '<unk>')) # Show the unmasked token\n",
    "\n",
    "print(f\"Decoded Masked Sequence: {' '.join(decoded_masked_tokens)}\")\n",
    "print(f\"Decoded Labels (masked words with original, unmasked words shown as is): {' '.join(decoded_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTbZGLqyHLvX"
   },
   "source": [
    "### Attention & Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vKFfSAhQqi4"
   },
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "  scores = torch.matmul(Q, K.transpose(-2,-1))\n",
    "  scores /= math.sqrt(Q.size(-1))\n",
    "  attn = F.softmax(scores, dim=-1)\n",
    "  return torch.matmul(attn, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNG2w6jMH9G_"
   },
   "source": [
    "### Multi-Head Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8xE5tA2H6kw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, heads):\n",
    "    super().__init__()\n",
    "    self.heads = heads\n",
    "    self.d_k = d_model // heads\n",
    "\n",
    "    self.q = nn.Linear(d_model, d_model)\n",
    "    self.k = nn.Linear(d_model, d_model)\n",
    "    self.v = nn.Linear(d_model, d_model)\n",
    "    self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C  = x.shape\n",
    "\n",
    "    Q = self.q(x).view(B, T, self.heads, self.d_k).transpose(1,2)\n",
    "    K = self.q(x).view(B, T, self.heads, self.d_k).transpose(1,2)\n",
    "    V = self.q(x).view(B, T, self.heads, self.d_k).transpose(1,2)\n",
    "\n",
    "    out = attention(Q, K, V)\n",
    "    out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "    return self.out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RxsvxC7I2yP"
   },
   "source": [
    "### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbI4Wlf8IyPi"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, d_model, heads):\n",
    "    super().__init__()\n",
    "    self.attn = MultiHeadAttention(d_model, heads)\n",
    "    self.ff = nn.Sequential(\n",
    "        nn.Linear(d_model, 4 * d_model),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(4 * d_model, d_model)\n",
    "    )\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.norm1(x + self.attn(x))\n",
    "    x = self.norm2(x + self.ff(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm2VgvM4JS0E"
   },
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9_TV0d1JRiX"
   },
   "outputs": [],
   "source": [
    "class MiniBERT(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model = 256, layers = 4, heads = 4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_emb = nn.Embedding(SEQ_LEN, d_model)\n",
    "\n",
    "    self.blocks = nn.ModuleList([\n",
    "        TransformerBlock(d_model, heads) for _ in range(layers)\n",
    "    ])\n",
    "\n",
    "    self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T = x.shape\n",
    "    pos = torch.arange(T, device=x.device)\n",
    "\n",
    "    x = self.token_emb(x) + self.pos_emb(pos)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "\n",
    "    return self.mlm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FFIEYlnKJIH"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWgMSmKWKCsu"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, loader):\n",
    "  model.train()\n",
    "  opt = torch.optim.AdamW(model.parameters(), lr = LR)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    # Wrap the loader with tqdm to display a progress bar\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    for tokens, labels in progress_bar:\n",
    "      tokens = tokens.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "\n",
    "      logits = model(tokens)\n",
    "      loss = F.cross_entropy(\n",
    "          logits.view(-1, VOCAB_SIZE),\n",
    "          labels.view(-1),\n",
    "          ignore_index = -100\n",
    "      )\n",
    "\n",
    "      opt.zero_grad()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "      progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} | Loss : {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2b5b6bc5c39e47f78043d9b5bb223a4b",
      "af362eb9c9ef4d7caeca4028d9ceef6d",
      "9346cfe4fb894921879b4bb03ce5036b",
      "b947f347871447dd84b62f22998a0b5e",
      "7dac9d7b012f40cbb05be698634ae004",
      "3715027ba53f4d149da45297c19f3d79",
      "5ef4d9924c9b4edc94ef90401a78903c",
      "2c13e791165f4919b33b16838e886d22",
      "912eebc4cb5f4ed5afdf36f04184b84d",
      "59ab0afffd714f3e83fce1b66b2c5afa",
      "330bdbdd9908406c972be288c769d1c6"
     ]
    },
    "id": "V_YF6nAJLAPM",
    "outputId": "9075b6b9-2af4-42bf-fe5a-a2559e692f65"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tokenizer = SimpleTokenizer(texts, VOCAB_SIZE)\n",
    "  dataset = MLMDataset(texts, tokenizer)\n",
    "  loader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "  model = MiniBERT(VOCAB_SIZE).to(DEVICE)\n",
    "  train(model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd93043f"
   },
   "source": [
    "### Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88c1f62f",
    "outputId": "c5193985-84b4-48f1-cb9a-ac99520a0896"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/BERT/mini_bert_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bba17d68"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog .\",\n",
    "    \"Artificial intelligence is transforming many industries around the world .\",\n",
    "    \"Natural language processing is a subfield of artificial intelligence .\",\n",
    "    \"The sun shines brightly in the summer sky .\",\n",
    "    \"Learning deep neural networks can be challenging but rewarding .\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b1f0ea8"
   },
   "source": [
    "### Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00efeec6"
   },
   "outputs": [],
   "source": [
    "def predict_masked_words(text, model, tokenizer, device, seq_len, vocab_size):\n",
    "    # a. Encode the input text and truncate/pad to SEQ_LEN - 2\n",
    "    tokens = tokenizer.encode(text)\n",
    "    tokens = tokens[:seq_len - 2]\n",
    "\n",
    "    # b. Add [CLS] and [SEP] tokens\n",
    "    cls_id = tokenizer.word2id[\"[CLS]\"]\n",
    "    sep_id = tokenizer.word2id[\"[SEP]\"]\n",
    "    mask_id = tokenizer.word2id[\"[MASK]\"]\n",
    "    unk_id = tokenizer.word2id[\"<unk>\"]\n",
    "\n",
    "    full_tokens = [\n",
    "        cls_id\n",
    "    ] + tokens + [\n",
    "        sep_id\n",
    "    ]\n",
    "\n",
    "    # c. Create a masked version and keep track of original tokens for masked positions\n",
    "    masked_input_tokens = list(full_tokens)\n",
    "    labels = [-100] * len(full_tokens) # -100 for ignore_index\n",
    "\n",
    "    # Only mask non-special tokens and within the SEQ_LEN limit\n",
    "    maskable_indices = [i for i, token_id in enumerate(full_tokens) if token_id not in [cls_id, sep_id, unk_id]]\n",
    "    random.shuffle(maskable_indices)\n",
    "\n",
    "    num_masks = int(len(maskable_indices) * 0.15)\n",
    "    masked_indices = maskable_indices[:num_masks]\n",
    "\n",
    "    for i in masked_indices:\n",
    "        original_token_id = full_tokens[i]\n",
    "        labels[i] = original_token_id # Store original ID as label\n",
    "\n",
    "        prob = random.random()\n",
    "        if prob < 0.8:\n",
    "            masked_input_tokens[i] = mask_id  # 80% change to [MASK]\n",
    "        elif prob < 0.9:\n",
    "            masked_input_tokens[i] = random.randint(0, vocab_size - 1)  # 10% change to random word\n",
    "        # 10% change to keep as is (original_token_id remains in masked_input_tokens[i])\n",
    "\n",
    "    # d. Pad to SEQ_LEN\n",
    "    padding = seq_len - len(masked_input_tokens)\n",
    "    if padding > 0:\n",
    "        masked_input_tokens += [tokenizer.word2id[\"[PAD]\"]] * padding\n",
    "        labels += [-100] * padding\n",
    "    else:\n",
    "        masked_input_tokens = masked_input_tokens[:seq_len]\n",
    "        labels = labels[:seq_len]\n",
    "\n",
    "    # e. Convert to torch.tensor and move to device\n",
    "    input_tensor = torch.tensor([masked_input_tokens], device=device)\n",
    "\n",
    "    # f. Pass through the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "\n",
    "    # g. Get predicted token IDs\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().cpu().tolist()\n",
    "\n",
    "    # h. Decode and print results\n",
    "    decoded_masked_input = []\n",
    "    predictions_str = []\n",
    "    for i, token_id in enumerate(input_tensor.squeeze().cpu().tolist()):\n",
    "        word = tokenizer.id2word.get(token_id, '<unk>')\n",
    "        decoded_masked_input.append(word)\n",
    "        if labels[i] != -100: # If it was a masked position\n",
    "            predicted_word = tokenizer.id2word.get(predicted_token_ids[i], '<unk>')\n",
    "            predictions_str.append(f\"({word} -> {predicted_word})\")\n",
    "\n",
    "    print(f\"Original Sentence: {text}\")\n",
    "    print(f\"Masked Input: {' '.join(decoded_masked_input)}\")\n",
    "    print(f\"Predictions for Masked Words: {' '.join(predictions_str)}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "792f01fd",
    "outputId": "956f6bac-f5c0-456a-de81-1abb3147c0ff"
   },
   "outputs": [],
   "source": [
    "model = MiniBERT(VOCAB_SIZE).to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating predictions for test sentences...\")\n",
    "for sentence in test_sentences:\n",
    "    predict_masked_words(sentence, model, tokenizer, DEVICE, SEQ_LEN, VOCAB_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2b5b6bc5c39e47f78043d9b5bb223a4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af362eb9c9ef4d7caeca4028d9ceef6d",
       "IPY_MODEL_9346cfe4fb894921879b4bb03ce5036b",
       "IPY_MODEL_b947f347871447dd84b62f22998a0b5e"
      ],
      "layout": "IPY_MODEL_7dac9d7b012f40cbb05be698634ae004"
     }
    },
    "2c13e791165f4919b33b16838e886d22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "330bdbdd9908406c972be288c769d1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3715027ba53f4d149da45297c19f3d79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59ab0afffd714f3e83fce1b66b2c5afa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ef4d9924c9b4edc94ef90401a78903c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7dac9d7b012f40cbb05be698634ae004": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "912eebc4cb5f4ed5afdf36f04184b84d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9346cfe4fb894921879b4bb03ce5036b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c13e791165f4919b33b16838e886d22",
      "max": 1085,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_912eebc4cb5f4ed5afdf36f04184b84d",
      "value": 1085
     }
    },
    "af362eb9c9ef4d7caeca4028d9ceef6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3715027ba53f4d149da45297c19f3d79",
      "placeholder": "​",
      "style": "IPY_MODEL_5ef4d9924c9b4edc94ef90401a78903c",
      "value": "Epoch 1/1: 100%"
     }
    },
    "b947f347871447dd84b62f22998a0b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59ab0afffd714f3e83fce1b66b2c5afa",
      "placeholder": "​",
      "style": "IPY_MODEL_330bdbdd9908406c972be288c769d1c6",
      "value": " 1085/1085 [32:17&lt;00:00,  1.52s/it, loss=6.38]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
