# ğŸ§  MiniBERT from Scratch (PyTorch)

A minimal implementation of **BERT (Bidirectional Encoder
Representations from Transformers)** built from scratch using PyTorch.

## ğŸš€ Features

-   Custom tokenizer (word-level)
-   Masked Language Modeling (MLM)
-   Multi-Head Self Attention
-   Transformer Encoder Blocks
-   Training + inference

## âš™ï¸ Architecture

``` mermaid
graph TD
    A[Input Tokens] --> B[Token Embedding]
    A --> C[Positional Embedding]
    B --> D[Add Embeddings]
    C --> D
    D --> E[Transformer Block x4]
    E --> F[MLM Head]
    F --> G[Vocab Predictions]
```

## ğŸ”¶ Transformer Block

``` mermaid
graph TD
    A[Input] --> B[Multi-Head Attention]
    B --> C[Add & Norm]
    C --> D[Feed Forward]
    D --> E[Add & Norm]
```

## ğŸ§ª Training

Masked Language Modeling with CrossEntropyLoss.

## âš ï¸ Notes

-   No attention mask
-   No segment embeddings
-   Basic tokenizer
-   Minor bug in attention (Q/K/V)

## â­

Star the repo if helpful!
